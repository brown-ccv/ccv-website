---
title: AI Tools
description: |
  Access to large language models and AI tools through CCV's computing infrastructure.
---

<ContentSection title="About ChatCCV Tools">
CCV provides access to cutting-edge AI tools and large language models through our high-performance computing
infrastructure. Whether you need to run LLMs for research, explore AI capabilities, or integrate AI into your
computational workflows, we have the resources and expertise to support your needs.

<Button
  variant="primary_outlined"
  size="md"
  href="https://docs.ccv.brown.edu/ai-tools/privacy-statement"
>
  CCV AI Tools Privacy Statement
</Button>

<CardGroup>
  <StyledCard iconName="FaFlask" title="LibreChat">
    LibreChat is an easy-to-use, open-source chat platform that lets you talk to multiple AI models in a private,
    customizable way. It's freely available to the Brown community and gives you full control over your data.

    <Button size="md" href="https://librechat.ccv.brown.edu/c/new">Try LibreChat</Button>

  </StyledCard>

  <StyledCard title="Transcribe">
     Transcribe audio files in over 20 different languages to text using state-of-the-art AI models such as Google
     Gemini and OpenAI Whisper.

     <Button size="md" href="https://ai.ccv.brown.edu/transcribe">Try Transcribe</Button>

  </StyledCard>
  <StyledCard iconName="FaFlask" title="Ask Oscar">
    Ask Oscar is an AI assistant that specializes in answering questions about Oscar, Brown University's HPC cluster. It has
    access to Oscar's documentation to help you find the information you need.

    <Button size="md" href="https://ai.ccv.brown.edu/retrieval">Try Ask Oscar</Button>

  </StyledCard>
</CardGroup>

**Note**: Ask Oscar is currently under development as a proof of concept. The information about the Oscar documentation
is not up-to-date and may be incorrect.

</ContentSection>

<ContentSection title="Ollama">

Ollama is the easiest way to run large language models directly on Brown’s Oscar high‑performance computing cluster.
Available as a preinstalled module, Ollama uses a simple client–server approach: you start a server on your allocated
GPU node, then connect a second terminal as the client to chat with models or launch jobs.

We at CCV host dozens of public, open‑weight models in a shared repository—such as Llama 4, DeepSeek‑r1, gpt‑oss, Mistral, and
Gemma3—so you don’t have to download anything; you simply point the OLLAMA_MODELS environment variable to CCV’s shared
path and run. We are also open to adding more models, so you can always contact us to request one!

You can choose default models or explicit variants (for example, requesting a larger parameter size), and interact
either through an interactive chat or programmatically.

For programmatic use, Ollama provides a lightweight Python client that lets you script evaluations, benchmarks, and
longer workflows across multiple models. Because LLMs are resource‑intensive, CCV recommends running on GPU nodes;
very large models may not fit in a single GPU’s memory and will otherwise spill to CPU, which is slower.
For those cases, requesting multiple GPUs allows Ollama to automatically shard model weights for better performance.


<ButtonGroup>
<Button href="https://docs.ccv.brown.edu/oscar/large-language-models/ollama">Ollama Documentation</Button>
<Button variant="primary_outlined" external={false} href="/services/oscar">About Oscar</Button>
</ButtonGroup>

</ContentSection>
